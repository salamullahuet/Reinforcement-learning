{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole game.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ercGi5QMZUhc"
      },
      "source": [
        "# This script makes use of OpenAI gym to train on the cartpole game.\n",
        "# Description of Game:\n",
        "\n",
        "# A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
        "# The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, \n",
        "# and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that \n",
        "# the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or \n",
        "# the cart moves more than 2.4 units from the center.\n",
        "# source : https://gym.openai.com/envs/CartPole-v1/\n",
        "\n",
        "\n",
        "# Parameters:\n",
        "\n",
        "# episodes - a number of games we want the agent to play.\n",
        "# gamma - aka decay or discount rate, to calculate the future discounted reward.\n",
        "# epsilon - aka exploration rate, this is the rate in which an agent randomly decides its action rather than prediction.\n",
        "# epsilon_decay - we want to decrease the number of explorations as it gets good at playing games.\n",
        "# epsilon_min - we want the agent to explore at least this amount.\n",
        "# learning_rate - Determines how much neural net learns in each iteration."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l35ceneqZqDH",
        "outputId": "2f7fbb75-5180-4ecd-f1fc-f7e6b8e57b0c"
      },
      "source": [
        "\n",
        "!pip install gym\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFlZwAH-Zxfz"
      },
      "source": [
        "### Inspired from the post of keon.io/deep-q-learning/\n",
        "\n",
        "Episodes = 1000\n",
        "\n",
        "'''\n",
        "By defining memory, we make sure that the state,action.reward and next_state\n",
        "are remembered, as the neural network in DQN tends to forget them after each \n",
        "iteration.\n",
        "'''\n",
        "\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=2000)\n",
        "    self.gamma = 0.95 # discount rate\n",
        "    self.epsilon = 1.0 #exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.learning_rate = 0.001 \n",
        "    self.model = self._build_model()\n",
        "    \n",
        "    \n",
        "  def _build_model(self):\n",
        "    '''\n",
        "    Neural Network for DQN\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim = self.state_size, activation = 'relu'))\n",
        "    model.add(Dense(24, activation = 'relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    \n",
        "    model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    '''\n",
        "    Keep appending the memory\n",
        "    '''\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "  def act(self, state):\n",
        "    '''\n",
        "    The agent will select at first it's action at random because \n",
        "    it is better for the agent to try all kinds of things before \n",
        "    it starts to see the patterns. \n",
        "    '''\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return random.randrange(self.action_size)\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0]) # argmax picks the highest value among\n",
        "    # the two values in act_values eg [0.67, 0.04]\n",
        "  \n",
        "  def replay(self, batch_size):\n",
        "    '''\n",
        "    Trains the neural net with experience in the memory.\n",
        "    We need to maximise the rewards in the long run, so we define gamma/discount\n",
        "    rate through which the agent will learn to maximise the discounted future \n",
        "    award in the long run.\n",
        "    '''\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "    for state, action, reward, next_state, done in minibatch:\n",
        "      target = reward # if done\n",
        "      \n",
        "      if not done:\n",
        "        target = (reward + self.gamma * np.amax(\n",
        "            self.model.predict(next_state)[0]))\n",
        "        target_f = self.model.predict(state)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state, target_f, epochs = 1, verbose = 0)\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "        \n",
        "  def load(self, name):\n",
        "    self.model.load_weghts(name)\n",
        "    \n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pOFy8wa5aEOD",
        "outputId": "2c5b9920-5ec8-4097-cc05-eeede40d4650"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  env = gym.make('CartPole-v1')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  agent = DQNAgent(state_size, action_size)\n",
        "  \n",
        "  done = False\n",
        "  batch_size = 32\n",
        "  \n",
        "  for a in range(Episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time in range(500):\n",
        "      \n",
        "      action = agent.act(state)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      reward = reward if not done else -10\n",
        "      next_state = np.reshape(next_state, [1,state_size])\n",
        "      agent.remember(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      if done:\n",
        "        print(\"Episode: {}/{}, score: {}, a : {:.2}\"\n",
        "             .format(a, Episodes, time, agent.epsilon))\n",
        "        break\n",
        "      if len(agent.memory) > batch_size:\n",
        "        agent.replay(batch_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0/1000, score: 23, a : 1.0\n",
            "Episode: 1/1000, score: 22, a : 0.11\n",
            "Episode: 2/1000, score: 9, a : 0.025\n",
            "Episode: 3/1000, score: 8, a : 0.01\n",
            "Episode: 4/1000, score: 9, a : 0.01\n",
            "Episode: 5/1000, score: 7, a : 0.01\n",
            "Episode: 6/1000, score: 9, a : 0.01\n",
            "Episode: 7/1000, score: 9, a : 0.01\n",
            "Episode: 8/1000, score: 9, a : 0.01\n",
            "Episode: 9/1000, score: 9, a : 0.01\n",
            "Episode: 10/1000, score: 7, a : 0.01\n",
            "Episode: 11/1000, score: 9, a : 0.01\n",
            "Episode: 12/1000, score: 9, a : 0.01\n",
            "Episode: 13/1000, score: 8, a : 0.01\n",
            "Episode: 14/1000, score: 9, a : 0.01\n",
            "Episode: 15/1000, score: 8, a : 0.01\n",
            "Episode: 16/1000, score: 8, a : 0.01\n",
            "Episode: 17/1000, score: 8, a : 0.01\n",
            "Episode: 18/1000, score: 10, a : 0.01\n",
            "Episode: 19/1000, score: 9, a : 0.01\n",
            "Episode: 20/1000, score: 8, a : 0.01\n",
            "Episode: 21/1000, score: 8, a : 0.01\n",
            "Episode: 22/1000, score: 9, a : 0.01\n",
            "Episode: 23/1000, score: 7, a : 0.01\n",
            "Episode: 24/1000, score: 8, a : 0.01\n",
            "Episode: 25/1000, score: 9, a : 0.01\n",
            "Episode: 26/1000, score: 9, a : 0.01\n",
            "Episode: 27/1000, score: 9, a : 0.01\n",
            "Episode: 28/1000, score: 9, a : 0.01\n",
            "Episode: 29/1000, score: 9, a : 0.01\n",
            "Episode: 30/1000, score: 7, a : 0.01\n",
            "Episode: 31/1000, score: 8, a : 0.01\n",
            "Episode: 32/1000, score: 9, a : 0.01\n",
            "Episode: 33/1000, score: 8, a : 0.01\n",
            "Episode: 34/1000, score: 8, a : 0.01\n",
            "Episode: 35/1000, score: 8, a : 0.01\n",
            "Episode: 36/1000, score: 9, a : 0.01\n",
            "Episode: 37/1000, score: 8, a : 0.01\n",
            "Episode: 38/1000, score: 8, a : 0.01\n",
            "Episode: 39/1000, score: 8, a : 0.01\n",
            "Episode: 40/1000, score: 8, a : 0.01\n",
            "Episode: 41/1000, score: 7, a : 0.01\n",
            "Episode: 42/1000, score: 9, a : 0.01\n",
            "Episode: 43/1000, score: 9, a : 0.01\n",
            "Episode: 44/1000, score: 9, a : 0.01\n",
            "Episode: 45/1000, score: 8, a : 0.01\n",
            "Episode: 46/1000, score: 8, a : 0.01\n",
            "Episode: 47/1000, score: 9, a : 0.01\n",
            "Episode: 48/1000, score: 8, a : 0.01\n",
            "Episode: 49/1000, score: 9, a : 0.01\n",
            "Episode: 50/1000, score: 9, a : 0.01\n",
            "Episode: 51/1000, score: 9, a : 0.01\n",
            "Episode: 52/1000, score: 9, a : 0.01\n",
            "Episode: 53/1000, score: 9, a : 0.01\n",
            "Episode: 54/1000, score: 8, a : 0.01\n",
            "Episode: 55/1000, score: 8, a : 0.01\n",
            "Episode: 56/1000, score: 8, a : 0.01\n",
            "Episode: 57/1000, score: 9, a : 0.01\n",
            "Episode: 58/1000, score: 9, a : 0.01\n",
            "Episode: 59/1000, score: 8, a : 0.01\n",
            "Episode: 60/1000, score: 8, a : 0.01\n",
            "Episode: 61/1000, score: 10, a : 0.01\n",
            "Episode: 62/1000, score: 8, a : 0.01\n",
            "Episode: 63/1000, score: 9, a : 0.01\n",
            "Episode: 64/1000, score: 7, a : 0.01\n",
            "Episode: 65/1000, score: 8, a : 0.01\n",
            "Episode: 66/1000, score: 9, a : 0.01\n",
            "Episode: 67/1000, score: 9, a : 0.01\n",
            "Episode: 68/1000, score: 8, a : 0.01\n",
            "Episode: 69/1000, score: 8, a : 0.01\n",
            "Episode: 70/1000, score: 8, a : 0.01\n",
            "Episode: 71/1000, score: 9, a : 0.01\n",
            "Episode: 72/1000, score: 9, a : 0.01\n",
            "Episode: 73/1000, score: 9, a : 0.01\n",
            "Episode: 74/1000, score: 10, a : 0.01\n",
            "Episode: 75/1000, score: 9, a : 0.01\n",
            "Episode: 76/1000, score: 8, a : 0.01\n",
            "Episode: 77/1000, score: 8, a : 0.01\n",
            "Episode: 78/1000, score: 8, a : 0.01\n",
            "Episode: 79/1000, score: 8, a : 0.01\n",
            "Episode: 80/1000, score: 10, a : 0.01\n",
            "Episode: 81/1000, score: 7, a : 0.01\n",
            "Episode: 82/1000, score: 7, a : 0.01\n",
            "Episode: 83/1000, score: 9, a : 0.01\n",
            "Episode: 84/1000, score: 9, a : 0.01\n",
            "Episode: 85/1000, score: 8, a : 0.01\n",
            "Episode: 86/1000, score: 7, a : 0.01\n",
            "Episode: 87/1000, score: 9, a : 0.01\n",
            "Episode: 88/1000, score: 7, a : 0.01\n",
            "Episode: 89/1000, score: 8, a : 0.01\n",
            "Episode: 90/1000, score: 9, a : 0.01\n",
            "Episode: 91/1000, score: 8, a : 0.01\n",
            "Episode: 92/1000, score: 8, a : 0.01\n",
            "Episode: 93/1000, score: 8, a : 0.01\n",
            "Episode: 94/1000, score: 8, a : 0.01\n",
            "Episode: 95/1000, score: 8, a : 0.01\n",
            "Episode: 96/1000, score: 9, a : 0.01\n",
            "Episode: 97/1000, score: 8, a : 0.01\n",
            "Episode: 98/1000, score: 8, a : 0.01\n",
            "Episode: 99/1000, score: 8, a : 0.01\n",
            "Episode: 100/1000, score: 9, a : 0.01\n",
            "Episode: 101/1000, score: 9, a : 0.01\n",
            "Episode: 102/1000, score: 7, a : 0.01\n",
            "Episode: 103/1000, score: 9, a : 0.01\n",
            "Episode: 104/1000, score: 8, a : 0.01\n",
            "Episode: 105/1000, score: 9, a : 0.01\n",
            "Episode: 106/1000, score: 7, a : 0.01\n",
            "Episode: 107/1000, score: 9, a : 0.01\n",
            "Episode: 108/1000, score: 11, a : 0.01\n",
            "Episode: 109/1000, score: 8, a : 0.01\n",
            "Episode: 110/1000, score: 8, a : 0.01\n",
            "Episode: 111/1000, score: 9, a : 0.01\n",
            "Episode: 112/1000, score: 8, a : 0.01\n",
            "Episode: 113/1000, score: 9, a : 0.01\n",
            "Episode: 114/1000, score: 9, a : 0.01\n",
            "Episode: 115/1000, score: 10, a : 0.01\n",
            "Episode: 116/1000, score: 8, a : 0.01\n",
            "Episode: 117/1000, score: 7, a : 0.01\n",
            "Episode: 118/1000, score: 9, a : 0.01\n",
            "Episode: 119/1000, score: 9, a : 0.01\n",
            "Episode: 120/1000, score: 8, a : 0.01\n",
            "Episode: 121/1000, score: 8, a : 0.01\n",
            "Episode: 122/1000, score: 9, a : 0.01\n",
            "Episode: 123/1000, score: 8, a : 0.01\n",
            "Episode: 124/1000, score: 8, a : 0.01\n",
            "Episode: 125/1000, score: 9, a : 0.01\n",
            "Episode: 126/1000, score: 9, a : 0.01\n",
            "Episode: 127/1000, score: 9, a : 0.01\n",
            "Episode: 128/1000, score: 8, a : 0.01\n",
            "Episode: 129/1000, score: 9, a : 0.01\n",
            "Episode: 130/1000, score: 8, a : 0.01\n",
            "Episode: 131/1000, score: 8, a : 0.01\n",
            "Episode: 132/1000, score: 8, a : 0.01\n",
            "Episode: 133/1000, score: 9, a : 0.01\n",
            "Episode: 134/1000, score: 9, a : 0.01\n",
            "Episode: 135/1000, score: 8, a : 0.01\n",
            "Episode: 136/1000, score: 8, a : 0.01\n",
            "Episode: 137/1000, score: 9, a : 0.01\n",
            "Episode: 138/1000, score: 8, a : 0.01\n",
            "Episode: 139/1000, score: 8, a : 0.01\n",
            "Episode: 140/1000, score: 8, a : 0.01\n",
            "Episode: 141/1000, score: 8, a : 0.01\n",
            "Episode: 142/1000, score: 8, a : 0.01\n",
            "Episode: 143/1000, score: 8, a : 0.01\n",
            "Episode: 144/1000, score: 9, a : 0.01\n",
            "Episode: 145/1000, score: 7, a : 0.01\n",
            "Episode: 146/1000, score: 8, a : 0.01\n",
            "Episode: 147/1000, score: 9, a : 0.01\n",
            "Episode: 148/1000, score: 9, a : 0.01\n",
            "Episode: 149/1000, score: 8, a : 0.01\n",
            "Episode: 150/1000, score: 8, a : 0.01\n",
            "Episode: 151/1000, score: 8, a : 0.01\n",
            "Episode: 152/1000, score: 8, a : 0.01\n",
            "Episode: 153/1000, score: 7, a : 0.01\n",
            "Episode: 154/1000, score: 8, a : 0.01\n",
            "Episode: 155/1000, score: 7, a : 0.01\n",
            "Episode: 156/1000, score: 8, a : 0.01\n",
            "Episode: 157/1000, score: 7, a : 0.01\n",
            "Episode: 158/1000, score: 9, a : 0.01\n",
            "Episode: 159/1000, score: 8, a : 0.01\n",
            "Episode: 160/1000, score: 8, a : 0.01\n",
            "Episode: 161/1000, score: 8, a : 0.01\n",
            "Episode: 162/1000, score: 8, a : 0.01\n",
            "Episode: 163/1000, score: 8, a : 0.01\n",
            "Episode: 164/1000, score: 9, a : 0.01\n",
            "Episode: 165/1000, score: 7, a : 0.01\n",
            "Episode: 166/1000, score: 9, a : 0.01\n",
            "Episode: 167/1000, score: 8, a : 0.01\n",
            "Episode: 168/1000, score: 9, a : 0.01\n",
            "Episode: 169/1000, score: 8, a : 0.01\n",
            "Episode: 170/1000, score: 9, a : 0.01\n",
            "Episode: 171/1000, score: 9, a : 0.01\n",
            "Episode: 172/1000, score: 9, a : 0.01\n",
            "Episode: 173/1000, score: 8, a : 0.01\n",
            "Episode: 174/1000, score: 9, a : 0.01\n",
            "Episode: 175/1000, score: 8, a : 0.01\n",
            "Episode: 176/1000, score: 8, a : 0.01\n",
            "Episode: 177/1000, score: 9, a : 0.01\n",
            "Episode: 178/1000, score: 9, a : 0.01\n",
            "Episode: 179/1000, score: 9, a : 0.01\n",
            "Episode: 180/1000, score: 8, a : 0.01\n",
            "Episode: 181/1000, score: 9, a : 0.01\n",
            "Episode: 182/1000, score: 7, a : 0.01\n",
            "Episode: 183/1000, score: 8, a : 0.01\n",
            "Episode: 184/1000, score: 9, a : 0.01\n",
            "Episode: 185/1000, score: 9, a : 0.01\n",
            "Episode: 186/1000, score: 7, a : 0.01\n",
            "Episode: 187/1000, score: 8, a : 0.01\n",
            "Episode: 188/1000, score: 9, a : 0.01\n",
            "Episode: 189/1000, score: 7, a : 0.01\n",
            "Episode: 190/1000, score: 9, a : 0.01\n",
            "Episode: 191/1000, score: 9, a : 0.01\n",
            "Episode: 192/1000, score: 7, a : 0.01\n",
            "Episode: 193/1000, score: 9, a : 0.01\n",
            "Episode: 194/1000, score: 8, a : 0.01\n",
            "Episode: 195/1000, score: 9, a : 0.01\n",
            "Episode: 196/1000, score: 9, a : 0.01\n",
            "Episode: 197/1000, score: 8, a : 0.01\n",
            "Episode: 198/1000, score: 8, a : 0.01\n",
            "Episode: 199/1000, score: 8, a : 0.01\n",
            "Episode: 200/1000, score: 8, a : 0.01\n",
            "Episode: 201/1000, score: 9, a : 0.01\n",
            "Episode: 202/1000, score: 9, a : 0.01\n",
            "Episode: 203/1000, score: 8, a : 0.01\n",
            "Episode: 204/1000, score: 7, a : 0.01\n",
            "Episode: 205/1000, score: 9, a : 0.01\n",
            "Episode: 206/1000, score: 8, a : 0.01\n",
            "Episode: 207/1000, score: 9, a : 0.01\n",
            "Episode: 208/1000, score: 9, a : 0.01\n",
            "Episode: 209/1000, score: 8, a : 0.01\n",
            "Episode: 210/1000, score: 8, a : 0.01\n",
            "Episode: 211/1000, score: 8, a : 0.01\n",
            "Episode: 212/1000, score: 8, a : 0.01\n",
            "Episode: 213/1000, score: 9, a : 0.01\n",
            "Episode: 214/1000, score: 9, a : 0.01\n",
            "Episode: 215/1000, score: 9, a : 0.01\n",
            "Episode: 216/1000, score: 9, a : 0.01\n",
            "Episode: 217/1000, score: 9, a : 0.01\n",
            "Episode: 218/1000, score: 8, a : 0.01\n",
            "Episode: 219/1000, score: 9, a : 0.01\n",
            "Episode: 220/1000, score: 8, a : 0.01\n",
            "Episode: 221/1000, score: 8, a : 0.01\n",
            "Episode: 222/1000, score: 8, a : 0.01\n",
            "Episode: 223/1000, score: 9, a : 0.01\n",
            "Episode: 224/1000, score: 8, a : 0.01\n",
            "Episode: 225/1000, score: 7, a : 0.01\n",
            "Episode: 226/1000, score: 9, a : 0.01\n",
            "Episode: 227/1000, score: 9, a : 0.01\n",
            "Episode: 228/1000, score: 9, a : 0.01\n",
            "Episode: 229/1000, score: 8, a : 0.01\n",
            "Episode: 230/1000, score: 8, a : 0.01\n",
            "Episode: 231/1000, score: 8, a : 0.01\n",
            "Episode: 232/1000, score: 7, a : 0.01\n",
            "Episode: 233/1000, score: 8, a : 0.01\n",
            "Episode: 234/1000, score: 8, a : 0.01\n",
            "Episode: 235/1000, score: 9, a : 0.01\n",
            "Episode: 236/1000, score: 9, a : 0.01\n",
            "Episode: 237/1000, score: 9, a : 0.01\n",
            "Episode: 238/1000, score: 7, a : 0.01\n",
            "Episode: 239/1000, score: 9, a : 0.01\n",
            "Episode: 240/1000, score: 9, a : 0.01\n",
            "Episode: 241/1000, score: 8, a : 0.01\n",
            "Episode: 242/1000, score: 8, a : 0.01\n",
            "Episode: 243/1000, score: 8, a : 0.01\n",
            "Episode: 244/1000, score: 9, a : 0.01\n",
            "Episode: 245/1000, score: 9, a : 0.01\n",
            "Episode: 246/1000, score: 9, a : 0.01\n",
            "Episode: 247/1000, score: 9, a : 0.01\n",
            "Episode: 248/1000, score: 10, a : 0.01\n",
            "Episode: 249/1000, score: 8, a : 0.01\n",
            "Episode: 250/1000, score: 8, a : 0.01\n",
            "Episode: 251/1000, score: 9, a : 0.01\n",
            "Episode: 252/1000, score: 9, a : 0.01\n",
            "Episode: 253/1000, score: 9, a : 0.01\n",
            "Episode: 254/1000, score: 10, a : 0.01\n",
            "Episode: 255/1000, score: 9, a : 0.01\n",
            "Episode: 256/1000, score: 9, a : 0.01\n",
            "Episode: 257/1000, score: 8, a : 0.01\n",
            "Episode: 258/1000, score: 9, a : 0.01\n",
            "Episode: 259/1000, score: 9, a : 0.01\n",
            "Episode: 260/1000, score: 9, a : 0.01\n",
            "Episode: 261/1000, score: 8, a : 0.01\n",
            "Episode: 262/1000, score: 9, a : 0.01\n",
            "Episode: 263/1000, score: 9, a : 0.01\n",
            "Episode: 264/1000, score: 8, a : 0.01\n",
            "Episode: 265/1000, score: 9, a : 0.01\n",
            "Episode: 266/1000, score: 7, a : 0.01\n",
            "Episode: 267/1000, score: 8, a : 0.01\n",
            "Episode: 268/1000, score: 8, a : 0.01\n",
            "Episode: 269/1000, score: 7, a : 0.01\n",
            "Episode: 270/1000, score: 9, a : 0.01\n",
            "Episode: 271/1000, score: 9, a : 0.01\n",
            "Episode: 272/1000, score: 9, a : 0.01\n",
            "Episode: 273/1000, score: 9, a : 0.01\n",
            "Episode: 274/1000, score: 8, a : 0.01\n",
            "Episode: 275/1000, score: 8, a : 0.01\n",
            "Episode: 276/1000, score: 8, a : 0.01\n",
            "Episode: 277/1000, score: 9, a : 0.01\n",
            "Episode: 278/1000, score: 8, a : 0.01\n",
            "Episode: 279/1000, score: 8, a : 0.01\n",
            "Episode: 280/1000, score: 8, a : 0.01\n",
            "Episode: 281/1000, score: 7, a : 0.01\n",
            "Episode: 282/1000, score: 7, a : 0.01\n",
            "Episode: 283/1000, score: 10, a : 0.01\n",
            "Episode: 284/1000, score: 9, a : 0.01\n",
            "Episode: 285/1000, score: 9, a : 0.01\n",
            "Episode: 286/1000, score: 9, a : 0.01\n",
            "Episode: 287/1000, score: 9, a : 0.01\n",
            "Episode: 288/1000, score: 8, a : 0.01\n",
            "Episode: 289/1000, score: 9, a : 0.01\n",
            "Episode: 290/1000, score: 9, a : 0.01\n",
            "Episode: 291/1000, score: 8, a : 0.01\n",
            "Episode: 292/1000, score: 9, a : 0.01\n",
            "Episode: 293/1000, score: 8, a : 0.01\n",
            "Episode: 294/1000, score: 10, a : 0.01\n",
            "Episode: 295/1000, score: 9, a : 0.01\n",
            "Episode: 296/1000, score: 9, a : 0.01\n",
            "Episode: 297/1000, score: 9, a : 0.01\n",
            "Episode: 298/1000, score: 8, a : 0.01\n",
            "Episode: 299/1000, score: 8, a : 0.01\n",
            "Episode: 300/1000, score: 9, a : 0.01\n",
            "Episode: 301/1000, score: 9, a : 0.01\n",
            "Episode: 302/1000, score: 8, a : 0.01\n",
            "Episode: 303/1000, score: 8, a : 0.01\n",
            "Episode: 304/1000, score: 7, a : 0.01\n",
            "Episode: 305/1000, score: 9, a : 0.01\n",
            "Episode: 306/1000, score: 9, a : 0.01\n",
            "Episode: 307/1000, score: 8, a : 0.01\n",
            "Episode: 308/1000, score: 9, a : 0.01\n",
            "Episode: 309/1000, score: 9, a : 0.01\n",
            "Episode: 310/1000, score: 9, a : 0.01\n",
            "Episode: 311/1000, score: 10, a : 0.01\n",
            "Episode: 312/1000, score: 8, a : 0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-beb3b8687992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-39876f6f117e>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4206\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4207\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4208\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4209\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3211\u001b[0m         \u001b[0;31m# places (like Keras) where the FuncGraph lives longer than the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m         \u001b[0;31m# ConcreteFunction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m         shared_func_graph=False)\n\u001b[0m\u001b[1;32m   3214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0;32m-> 1556\u001b[0;31m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[1;32m   1557\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    614\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[1;32m    615\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_EagerDefinedFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_on_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36madd_function\u001b[0;34m(fdef)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m   \u001b[0;34m\"\"\"Add a function definition to the context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36madd_function\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \"\"\"\n\u001b[1;32m   1123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ContextAddFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_function_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "kljgM7K9a33Z",
        "outputId": "1e14988b-bd61-4ad4-fe41-5c6a1038047e"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "EPISODES = 1000\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500):\n",
        "            # env.render()\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        # if e % 10 == 0:\n",
        "        #     agent.save(\"./save/cartpole-dqn.h5\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/1000, score: 28, e: 1.0\n",
            "episode: 1/1000, score: 34, e: 0.86\n",
            "episode: 2/1000, score: 15, e: 0.79\n",
            "episode: 3/1000, score: 19, e: 0.72\n",
            "episode: 4/1000, score: 12, e: 0.68\n",
            "episode: 5/1000, score: 10, e: 0.65\n",
            "episode: 6/1000, score: 16, e: 0.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-854419f13a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;31m# if e % 10 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m#     agent.save(\"./save/cartpole-dqn.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-854419f13a45>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 target = (reward + self.gamma *\n\u001b[1;32m     49\u001b[0m                           np.amax(self.model.predict(next_state)[0]))\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    380\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     ))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \"\"\"\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3136\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3138\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"normalize_element\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Imported here to avoid circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4b_OLnL_nKP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}